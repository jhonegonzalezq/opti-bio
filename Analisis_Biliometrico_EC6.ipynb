{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhonegonzalezq/opti-bio/blob/main/Analisis_Biliometrico_EC6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1g1wJR5QixG-7LhTavm5Egpk34TU1JfuD?usp=sharing)\n"
      ],
      "metadata": {
        "id": "DwdXkH3xmEOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "############################################################################\n",
        "\n",
        " Created by: Prof. Valdecy Pereira, D.Sc.\n",
        " UFF - Universidade Federal Fluminense (Brazil)\n",
        " email:  valdecy.pereira@gmail.com\n",
        " pyBibX - A Bibliometric and Scientometric Library\n",
        " Example - Scopus\n",
        "\n",
        " Citation:\n",
        " PEREIRA, V.; BASILIO, M.P.; SANTOS, C.H.T. (2025). PyBibX: A Python Library for Bibliometric and\n",
        " Scientometric Analysis Powered with Artificial Intelligence Tools. Data Technologies and Applications.\n",
        " Vol. ahead-of-print No. ahead-of-print. doi: https://doi.org/10.1108/DTA-08-2023-0461\n",
        "\n",
        "############################################################################"
      ],
      "metadata": {
        "id": "c5lGII_ZljK8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ip99bnK3_BoK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "49dcdf4f-a35a-4b98-bfa0-d485957219af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pybibx in /usr/local/lib/python3.11/dist-packages (5.2.2)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (from pybibx) (0.17.3)\n",
            "Requirement already satisfied: bert-extractive-summarizer in /usr/local/lib/python3.11/dist-packages (from pybibx) (0.10.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from pybibx) (5.2.0)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.11/dist-packages (from pybibx) (0.8.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (from pybibx) (4.3.3)\n",
            "Requirement already satisfied: keybert in /usr/local/lib/python3.11/dist-packages (from pybibx) (0.9.0)\n",
            "Requirement already satisfied: llmx in /usr/local/lib/python3.11/dist-packages (from pybibx) (0.0.21a0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from pybibx) (3.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pybibx) (3.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from pybibx) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pybibx) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from pybibx) (2.2.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from pybibx) (11.3.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from pybibx) (5.24.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pybibx) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pybibx) (1.6.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from pybibx) (0.2.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from pybibx) (5.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pybibx) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from pybibx) (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from pybibx) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from pybibx) (4.55.2)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (from pybibx) (0.5.9.post2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from pybibx) (1.99.9)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (from pybibx) (1.9.4)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from bert-extractive-summarizer->pybibx) (3.8.7)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.11/dist-packages (from bertopic->pybibx) (0.8.40)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.11/dist-packages (from bertopic->pybibx) (4.67.1)\n",
            "Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.11/dist-packages (from bertopic->pybibx) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->pybibx) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->pybibx) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->pybibx) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->pybibx) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly->pybibx) (25.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pybibx) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pybibx) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->pybibx) (0.34.4)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers->pybibx) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (3.18.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pybibx) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pybibx) (1.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->pybibx) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->pybibx) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->pybibx) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->pybibx) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->pybibx) (0.6.2)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn->pybibx) (0.5.13)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim->pybibx) (7.3.0.post1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai->pybibx) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.11/dist-packages (from google-generativeai->pybibx) (2.25.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (from google-generativeai->pybibx) (2.179.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from google-generativeai->pybibx) (2.38.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from google-generativeai->pybibx) (5.29.5)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from google-generativeai->pybibx) (2.11.7)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai->pybibx) (1.26.1)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert->pybibx) (13.9.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llmx->pybibx) (0.11.0)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.11/dist-packages (from llmx->pybibx) (5.6.3)\n",
            "Requirement already satisfied: cohere in /usr/local/lib/python3.11/dist-packages (from llmx->pybibx) (5.17.0)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from llmx->pybibx) (0.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pybibx) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pybibx) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pybibx) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pybibx) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->pybibx) (3.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->pybibx) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->pybibx) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->pybibx) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->pybibx) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->pybibx) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai->pybibx) (3.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core->google-generativeai->pybibx) (1.70.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai->pybibx) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai->pybibx) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=2.15.0->google-generativeai->pybibx) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->pybibx) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->pybibx) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->pybibx) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->pybibx) (1.1.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai->pybibx) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai->pybibx) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->google-generativeai->pybibx) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->pybibx) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->pybibx) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->pybibx) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert->pybibx) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert->pybibx) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim->pybibx) (1.17.3)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere->llmx->pybibx) (1.12.0)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere->llmx->pybibx) (0.4.0)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere->llmx->pybibx) (2.32.4.20250809)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai->pybibx) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai->pybibx) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client->google-generativeai->pybibx) (4.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pybibx) (3.0.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer->pybibx) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer->pybibx) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer->pybibx) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer->pybibx) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer->pybibx) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer->pybibx) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer->pybibx) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer->pybibx) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer->pybibx) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer->pybibx) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer->pybibx) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy->bert-extractive-summarizer->pybibx) (3.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer->llmx->pybibx) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->llmx->pybibx) (1.5.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai->pybibx) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai->pybibx) (1.71.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer->pybibx) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert->pybibx) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai->pybibx) (0.6.1)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->bert-extractive-summarizer->pybibx) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->bert-extractive-summarizer->pybibx) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->bert-extractive-summarizer->pybibx) (0.21.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->bert-extractive-summarizer->pybibx) (1.2.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pybibx\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall numpy==1.26.4 pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JzC4t44cHDwQ",
        "outputId": "b91fc635-29f6-414b-c750-cc2938194b08"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 pandas-2.3.1 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "numpy",
                  "six"
                ]
              },
              "id": "6649e16a8a704fb98709eea1ea992f3c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbWNjCfN_BoK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Dowload .bib file\n",
        "!wget  scopus.bib https://github.com/jorgeiv500/opti-bio/raw/main/scopus263.bib # se puede cambiar por scopus_1, scopus_2, scopus_3\n",
        "#!wget https://github.com/Valdecy/pyBibX/raw/main/assets/bibs/scopus.bib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl-I-RZf_BoK"
      },
      "outputs": [],
      "source": [
        "# Required Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import textwrap\n",
        "\n",
        "from google.colab import data_table\n",
        "from tabulate import tabulate\n",
        "from prettytable import PrettyTable\n",
        "from pybibx.base import pbx_probe"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset\n",
        "---\n",
        "In this section, we will load and inspect the dataset."
      ],
      "metadata": {
        "id": "jwJe6Ss4Z1vA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9orhNMeb_BoL"
      },
      "outputs": [],
      "source": [
        "# Load .bib\n",
        "# Arguments: file_bib = 'filename.bib'; db = 'scopus', 'wos', 'pubmed'; del_duplicated = True, False\n",
        "file_name = 'scopus263.bib'\n",
        "database  = 'scopus'\n",
        "bibfile   = pbx_probe(file_bib = file_name, db = database, del_duplicated = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Health Analysis\n",
        "health = bibfile.health_bib()\n",
        "\n",
        "# Check Health\n",
        "health"
      ],
      "metadata": {
        "id": "YZbDYiOPT5ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate EDA (Exploratory Data Analysis) Report\n",
        "report  = bibfile.eda_bib()\n",
        "\n",
        "# Check Report\n",
        "report"
      ],
      "metadata": {
        "id": "aEykvk6U_BoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The metadata can be reviewed and manually modified. If you need to make adjustments, you can directly edit the bibfile.data, which is a DataFrame containing all the utilized information.\n",
        "print(tabulate(bibfile.data.head(n = 10), headers = 'keys', tablefmt = 'psql'))\n",
        "# Modify 'bibfile.data' as needed."
      ],
      "metadata": {
        "id": "coOr0BIuwgZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Docs IDs\n",
        "data_table.DataTable(bibfile.table_id_doc, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "1AJOsY2W_BoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Docs IDs per Type\n",
        "data_table.DataTable(bibfile.id_doc_types(), num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "Sgz81QTf_BoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Authors IDs\n",
        "data_table.DataTable(bibfile.table_id_aut, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "PyjG-IRf_BoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# These indices are local (considers only the .bib scope)\n",
        "# H-index measures their academic impact by identifying the number of papers (h) that have each received at least h citations\n",
        "# E-Index quatifies excess citations within the H-core revealing \"hidden\" impact beyond the H-index threshold.\n",
        "# The G-Index emphasizes highly cited work, making it sensitive to breakthrough publications.\n",
        "# The M-Index contextualizes the H-index by normalizing it over the researcher’s career duration\n",
        "aut_m = bibfile.m_index(2022)\n",
        "df_idx = {\n",
        "    'Author': bibfile.u_aut,\n",
        "    'H-index': bibfile.aut_h,\n",
        "    'E-Index': bibfile.aut_e,\n",
        "    'G-Index': bibfile.aut_g,\n",
        "    'M-Index': aut_m\n",
        "}\n",
        "\n",
        "df_idx = pd.DataFrame(df_idx)\n",
        "df_idx"
      ],
      "metadata": {
        "id": "PKKE_AIYLmta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Sources IDs\n",
        "data_table.DataTable(bibfile.table_id_jou, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "DpfZQaSl_BoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Institutions IDs\n",
        "data_table.DataTable(bibfile.table_id_uni, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "aNogO6Xs_BoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Countries IDs\n",
        "data_table.DataTable(bibfile.table_id_ctr, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "bXdCo57A_BoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Authors Keywords IDs\n",
        "data_table.DataTable(bibfile.table_id_kwa, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "zh2p-5Lz_BoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Keywords Plus IDs\n",
        "data_table.DataTable(bibfile.table_id_kwp, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "Htt7Oln8_BoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Reference IDs (If References are Given)\n",
        "df = pd.DataFrame({'Reference': bibfile.u_ref, 'Reference ID': bibfile.u_ref_id})\n",
        "data_table.DataTable(df, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "2Xc_4QLAyHsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis\n",
        "---\n",
        "In this section, we will perform EDA"
      ],
      "metadata": {
        "id": "oQ-a26oMbkWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WordCloud from the Abstracts, Title, Authors Keywords or Keywords Plus\n",
        "# Arguments: entry             = 'abs', 'title', 'kwa', or 'kwp'\n",
        "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
        "bibfile.word_cloud_plot(entry = 'abs', size_x = 15, size_y = 10, wordsn = 500, rmv_custom_words = [])"
      ],
      "metadata": {
        "id": "oiQdiGsN_BoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Table\n",
        "num_columns = 5\n",
        "data_wd     = bibfile.ask_gpt_wd\n",
        "items       = list(data_wd.items())\n",
        "\n",
        "field_names = []\n",
        "for i in range(num_columns):\n",
        "    field_names.append(f\"Word {i + 1}\")\n",
        "    field_names.append(f\"Importance {i + 1}\")\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = field_names\n",
        "\n",
        "for i in range(0, len(items), num_columns):\n",
        "    row = []\n",
        "    for j in range(num_columns):\n",
        "        if i + j < len(items):\n",
        "            word, importance = items[i + j]\n",
        "            row.extend([word, round(importance, 4)])\n",
        "        else:\n",
        "            row.extend([\"\", \"\"])\n",
        "    table.add_row(row)\n",
        "\n",
        "# Print the table\n",
        "print(table)"
      ],
      "metadata": {
        "id": "Z6T5WT7CVYzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-Grams\n",
        "# Arguments: view       = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            entry      = 'abs', 'title', 'kwa', or 'kwp'\n",
        "#            n_grams    = An integer with size n (representing the most common groups of words with size n)\n",
        "#            stop_words = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
        "#                         'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
        "#                         'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Portuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
        "#                         'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
        "#            rmv_custom_words  = A list of custom stopwords to clean the corpus\n",
        "#             wordsn           = Number of N-Grams\n",
        "bibfile.get_top_ngrams(view = 'notebook', entry = 'kwp', ngrams = 3, stop_words = [], rmv_custom_words = [], wordsn = 15)"
      ],
      "metadata": {
        "id": "U82tvIvb_BoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Table\n",
        "data_ng = bibfile.ask_gpt_ng\n",
        "data_table.DataTable(data_ng, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "0FaapNGV5bM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Documents Projection based on Words. (An interactive plot). It returns the Projection (each document coordinate) and the Labels (each document cluster)\n",
        "# Arguments: view              = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            corpus_type       = 'abs', 'title', 'kwa', or 'kwp';\n",
        "#            stop_words        = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
        "#                                'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
        "#                                'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
        "#                                'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
        "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
        "#            custom_label      = A list of custom labels for each document. The user can define each document cluster;\n",
        "#            custom_projection = A list of custom coordinates for each document. The user can define each document coordinate;\n",
        "#            n_components      = Number of Dimensions;\n",
        "#            n_clusters        = Only relevant if  cluster_method = 'kmeans'.Number of Clusters.;\n",
        "#            node_labels       = If True, labels appear in nodes;\n",
        "#            node_size         = Node size;\n",
        "#            node_font_size    = Node font size;\n",
        "#            tf_idf            = True or False (True -> The Cluster Algorithm will use the DTM to calculate each document Label. False -> The Cluster Algorithm will use the Coordinates to calculate each document Label);\n",
        "#            embeddings        = True or False (True -> The Cluster Algorithm will use the Word Embeddings to calculate each document Label. False -> The Cluster Algorithm will use the Coordinates to calculate each document Label);\n",
        "#            model             = Only relevant if 'embeddings = True'. Specifies the used AI model. The default value is 'allenai/scibert_scivocab_uncased';\n",
        "#            method            = 'tsvd' or 'umap' ('tsvd' -> Truncated SVD projection method is used. 'umap' -> UMAP projection method is used);\n",
        "#            showlegend        = If True, shows legend;\n",
        "#            cluster_method    = 'kmeans' or 'hdbscan';\n",
        "#            min_size          = Only relevant if  cluster_method = 'hdbscan'. Minimum number of elements in a Cluster;\n",
        "#            max_size          = Only relevant if  cluster_method = 'hdbscan'. Maximum number of elements in a Cluster\n",
        "projection, labels = bibfile.docs_projection(view              = 'notebook',\n",
        "                                             corpus_type       = 'abs',\n",
        "                                             stop_words        = ['en'],\n",
        "                                             rmv_custom_words  = [],\n",
        "                                             custom_label      = [],\n",
        "                                             custom_projection = [],\n",
        "                                             n_components      = 2,\n",
        "                                             n_clusters        = 6,\n",
        "                                             node_labels       = True,\n",
        "                                             node_size         = 12,\n",
        "                                             node_font_size    = 5,\n",
        "                                             tf_idf            = False,\n",
        "                                             embeddings        = False,\n",
        "                                             model             = 'allenai/scibert_scivocab_uncased',\n",
        "                                             method            = 'umap',\n",
        "                                             showlegend        = True,\n",
        "                                             cluster_method    = 'kmeans',\n",
        "                                             min_size          = 5,\n",
        "                                             max_size          = 50\n",
        "                                             )"
      ],
      "metadata": {
        "id": "xkiiRftr_BoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Table\n",
        "data_pr = pd.DataFrame(np.hstack([projection, labels.reshape(-1,1)]))\n",
        "data_table.DataTable(data_pr, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "ywT35wBe54q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Articles per Cluster\n",
        "cluster      = 4\n",
        "idx_articles = [i for i in range(0, labels.shape[0]) if labels[i] == cluster]\n",
        "print(*idx_articles, sep = ', ')"
      ],
      "metadata": {
        "id": "hCt6nL7r_BoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arguments: view              = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            key               = 'abs', 'title', 'jou, 'kwa', or 'kwp';\n",
        "#            stop_words        = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
        "#                                'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
        "#                                'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
        "#                                'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
        "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
        "#            topn              = Total number entities;\n",
        "#            txt_font_size     = Font size of the text inside the bins;\n",
        "#            start             = Start Year; -1 = all years\n",
        "#            end               = End Year;   -1 = all years\n",
        "bibfile.plot_evolution_year(view             = 'notebook',\n",
        "                            stop_words       = [],\n",
        "                            rmv_custom_words = [],\n",
        "                            key              = 'kwa',\n",
        "                            topn             = 10,\n",
        "                            txt_font_size    = 12,\n",
        "                            start            = 2010,\n",
        "                            end              = 2025)"
      ],
      "metadata": {
        "id": "YJ3JjjKp_BoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Table\n",
        "data_ep = bibfile.ask_gpt_ep\n",
        "print(textwrap.fill(data_ep, 150))"
      ],
      "metadata": {
        "id": "6R6XmiHW7nz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arguments: view              = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            topn              = Most frequent n words\n",
        "#            custom            = Plot specific words\n",
        "bibfile.plot_evolution_year_complement(data_ep, view = 'notebook', topn = 20, custom = [])"
      ],
      "metadata": {
        "id": "fkOJOTihWVs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sankey Diagram (An interactive plot)\n",
        "# Arguments:\n",
        "#   view         : Determines the rendering mode.'notebook' -> Plots in your preferred Notebook app. 'browser'  -> Plots in your preferred browser window.\n",
        "#   entry        : A list defining the sequence of data columns to be visualized. Allowed keys: 'aut', 'cout', 'inst', 'jou', 'kwa', 'kwp', 'lan'\n",
        "#   rmv_unknowns : Boolean flag controlling how unknown entries are handled. True  -> Remove any relationships that include 'unknown'.\n",
        "#   topn         : Specifies the maximum number of top entities/connections to display at each level. Set to \"None\" for no filtering or provide a list with limits corresponding to each connection between the nodes defined in 'entry'.\n",
        "bibfile.sankey_diagram(view = 'notebook', entry = ['aut', 'cout', 'lan'], topn = [3, 5], rmv_unknowns = True)\n",
        "\n",
        "# PS: The white bars can be dragged"
      ],
      "metadata": {
        "id": "nuUYvtG2_BoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Table\n",
        "data_sk = bibfile.ask_gpt_sk\n",
        "data_table.DataTable(data_sk, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "DsTaS_h18Gzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Count Y per X (An interactive bar chart)\n",
        "# Arguments:\n",
        "#   view          : Determines where the plot will be rendered. 'browser'  -> Plots in your preferred browser window. 'notebook' -> (if supported) Plots within your notebook environment.\n",
        "#   rmv_unknowns  : Boolean flag to handle unknown entries. True  -> Removes any relationships containing unknown values.\n",
        "#   x             : The key (column) to use for the X-axis. Must be one of: 'aut', 'cout', 'inst', 'jou', 'kwa', 'kwp', 'lan'. Default is 'cout'.\n",
        "#   y             : The key (column) to use for the Y-axis. Must be one of: 'aut', 'cout', 'inst', 'jou', 'kwa', 'kwp', 'lan'. Default is 'aut'.\n",
        "#   topn_x        : Maximum number of top X categories (based on total counts) to display. Default is 5.\n",
        "#   topn_y        : Maximum number of top Y entries (per X category) to display. Default is 5.\n",
        "#   text_font_size: Font size for the text labels displayed inside the bars. Default is 12.\n",
        "#   x_angle       : Angle (in degrees) for the X-axis tick labels. Default is -90 (rotating labels vertically).\n",
        "bibfile.plot_count_y_per_x(view = 'notebook', rmv_unknowns = True, x = 'cout', y = 'aut', topn_x = 5, topn_y = 5, text_font_size = 12, x_angle = -90)"
      ],
      "metadata": {
        "id": "nbIJzq_XyDL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Table\n",
        "data_table.DataTable(bibfile.top_y_x, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "tr1O5D5CyEc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tree Map\n",
        "# Arguments: view          = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            entry         = 'kwp', 'kwa', 'aut', 'jou', 'ctr', or 'inst';\n",
        "#            topn          = Total number entities\n",
        "#            txt_font_size = Font size of the text inside the bins;\n",
        "bibfile.tree_map(view = 'notebook', entry = 'jou', topn = 30)"
      ],
      "metadata": {
        "id": "RQZUFDvF_BoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authors Productivity Plot (An interactive plot). It informs for each year the documents (IDs) published for each author\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            topn = Total number entities\n",
        "bibfile.authors_productivity(view = 'notebook', topn = 20)"
      ],
      "metadata": {
        "id": "IP_DGzqT_BoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Table\n",
        "data_ap = bibfile.ask_gpt_ap\n",
        "data_table.DataTable(data_ap, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "YqYGKZOH8Zv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Countries Productivity Plot (An interactive plot). It informs the production for each Country (count is made considering each doc author)\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "bibfile.countries_productivity(view = 'notebook')"
      ],
      "metadata": {
        "id": "0boOYx0PU8l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Table\n",
        "data_cp = bibfile.ask_gpt_cp\n",
        "data_table.DataTable(data_cp, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "p3RqTVNIF0Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Institutions Productivity Plot (An interactive plot). It informs for each year the documents (IDs) published for each institution\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            topn = Total number entities\n",
        "bibfile.institution_productivity(view = 'notebook', topn = 20)"
      ],
      "metadata": {
        "id": "4jfag87MGIDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Table\n",
        "data_ip = bibfile.ask_gpt_ip\n",
        "data_table.DataTable(data_ip, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "xd5-qy8uGOVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sources Productivity Plot (An interactive plot). It informs for each year the documents (IDs) published for each source (journal)\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            topn = Total number entities\n",
        "bibfile.source_productivity(view = 'notebook', topn = 20)"
      ],
      "metadata": {
        "id": "u0rpeJTAGT1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Table\n",
        "data_sp = bibfile.ask_gpt_sp\n",
        "data_table.DataTable(data_sp, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "XpXLzmaXGZVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar Plots\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#                    statistic = 'dpy', 'cpy', 'ppy', 'ltk', 'spd', 'spc', 'apd', 'apc', 'aph', 'bdf_1', 'bdf_2', 'bdf_3', 'ipd', 'ipc', 'cpd', 'cpc', 'lpd', 'kpd', 'kad'\n",
        "#                        'dpy' = Documents per Year\n",
        "#                         cpy' = Citations per Year\n",
        "#                        'ppy' = Past Citations per Year\n",
        "#                        'ltk' = Lotka's Law\n",
        "#                        'spd' = Sources per Documents\n",
        "#                        'spc' = Sources per Citations\n",
        "#                        'apd' = Authors per Documents\n",
        "#                        'apc' = Authors per Citations\n",
        "#                        'aph' = Authors per H-Index\n",
        "#                        'bdf_1', 'bdf_2', 'bdf_3' = Bradford's Law - Core Sources 1, 2 or 3\n",
        "#                        'ipd' = Institutions per Documents\n",
        "#                        'ipc' = Institutions per Citations\n",
        "#                        'cpd' = Countries per Documents\n",
        "#                        'cpc' = Countries per Citations\n",
        "#                        'lpd' = Language per Documents\n",
        "#                        'kpd' = Keywords Plus per Documents\n",
        "#                        'kad' = Authors' Keywords per Documents\n",
        "#                         topn = Total number entities\n",
        "bibfile.plot_bars(view = 'notebook', statistic = 'apd', topn = 20)"
      ],
      "metadata": {
        "id": "pZrovn05_BoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Table\n",
        "data_bp = bibfile.ask_gpt_bp\n",
        "data_table.DataTable(data_bp, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "jxihXvY08zE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Top References\n",
        "# Arguments: view       = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            topn       = Specifies the number of top references (by citation count) to display in the plot;\n",
        "#            font_size  = Controls the font size of the legend in the plot;\n",
        "#            use_ref_id = Chooses between using reference names or reference IDs when processing and plotting data;\n",
        "#            date_start = If provided, shows citing articles with a publication year greater than or equal to this value are included. If None, then there is no lower limit on the publication year.\n",
        "#            date_end   = If provided, shows citing articles with a publication year less than or equal to this value are included. If None, then there is no upprer limit on the publication year.\n",
        "bibfile.plot_top_refs(view = 'notebook', topn = 10, font_size = 10, use_ref_id = False, date_start = None, date_end = None)"
      ],
      "metadata": {
        "id": "2Mchuaz1zPSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Top References\n",
        "data_table.DataTable(bibfile.top_refs, num_rows_per_page = 15)\n",
        "\n",
        "# PS: Optionally, use 'bibfile.merge_reference(get = [], replace_for = 'name')' to correct References\n",
        "# wrong_references = [\n",
        "#                     'Mousseau, V., Slowinski, R., Inferring an ELECTRE TRI model from assignment examples (1998) Journal of Global Optimization, 12, pp. 157-174',\n",
        "#                     'Mousseau, V., Słowiński, R., Inferring an ELECTRE TRI model from assignment examples (1998) Journal of Global Optimization, 12 (2), pp. 157-174'\n",
        "#                    ]\n",
        "# correct_reference = 'Mousseau, V., Slowinski, R., Inferring an ELECTRE TRI model from assignment examples (1998) Journal of Global Optimization, 12 (2), pp. 157-174'\n",
        "# bibfile.merge_reference(get = wrong_references, replace_for = correct_reference)"
      ],
      "metadata": {
        "id": "4_j_VzAizgiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Citation Trajectory\n",
        "# Arguments: view     = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#           ref_names = If provided (i.e., nonempty), the function uses these reference names to track and plot citation trajectories;\n",
        "#           ref_ids   = If ref_names is empty but ref_ids is provided, the function will use these IDs instead.\n",
        "bibfile.plot_citation_trajectory(view = 'notebook', ref_names = [], ref_ids = ['r_56','r_66', 'r_57'])\n"
      ],
      "metadata": {
        "id": "zuM7TdQU2JXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# References Citation Matrix\n",
        "# Arguments: tgt_ref_id = For a nonempty list, show olny the references in the list. An empty list (the default), shows all references;\n",
        "#            date_start = If provided, shows citing articles with a publication year greater than or equal to this value are included. If None, then there is no lower limit on the publication year;\n",
        "#            date_end   = If provided, shows citing articles with a publication year less than or equal to this value are included. If None, then there is no upprer limit on the publication year.\n",
        "ct_matrix = bibfile.ref_citation_matrix(tgt_ref_id = [], date_start = None, date_end = None)\n",
        "\n",
        "# Check Citation Matrix. Citing Articles are in the format (Article ID, Year)\n",
        "ct_matrix"
      ],
      "metadata": {
        "id": "wdD0kBbH2SmC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RPYS (Reference Publication Year Spectroscopy) with Gaussian Filter to Find Peaks\n",
        "# Arguments: view       = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            peaks_only = If False, the function plots data for all publication years. If True, shows only peaks.\n",
        "bibfile.plot_rpys(view = 'notebook', peaks_only = False)\n",
        "\n",
        "# PS: Use the Slider to focus in a range of years"
      ],
      "metadata": {
        "id": "CtfbHK4u2yZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check RPYS Data\n",
        "data_table.DataTable(bibfile.rpys_rs, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "BhTmrlUP2_Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check RPYS Data Peaks\n",
        "data_table.DataTable(bibfile.rpys_pk, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "8zrk4pFp2_ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Analysis\n",
        "---\n",
        "In this section, we will perform Network Analysis"
      ],
      "metadata": {
        "id": "uX5HPBMkcGR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# References Top Cited Co-References\n",
        "# Arguments: groups = controls how many references are considered together;\n",
        "#            topn   = Specifies the number of top references (by citation count) to display in the plot;\n",
        "co_cited = bibfile.top_cited_co_references(group = 2, topn = 10)\n",
        "co_cited"
      ],
      "metadata": {
        "id": "Vm4eoO6hQ7Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Co-Citation Network\n",
        "# Arguments:  view       = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#             tgt_ref_id = The reference ID;\n",
        "#             topn       = Specifies the number of top references (by citation count) to display in the plot;\n",
        "#bibfile.plot_co_citation_network(view = 'notebook', target_ref_id = 'r_5607', topn = 10)\n",
        "bibfile.plot_co_citation_network(view='notebook', target_ref_id =['r_56'], topn=20)"
      ],
      "metadata": {
        "id": "PKEfXEABRBXf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Co-Citation Network\n",
        "data_table.DataTable(bibfile.top_co_c, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "a415z7JxRK6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arguments:\n",
        "# entry      = 'aut', 'cout', 'inst', 'kwa', or 'kwp'.\n",
        "# tgt        = List of specific names.\n",
        "# topn       = Integer. Specifies the number of top authors to display based on their total contributions.\n",
        "# rows       = Integer. Defines the number of rows in the subplot grid for the visual layout.\n",
        "# cols       = Integer. Defines the number of columns in the subplot grid for the visual layout.\n",
        "# wspace     = Float. Adjusts horizontal spacing between subplots.\n",
        "# hspace     = Float. Adjusts vertical spacing between subplots.\n",
        "# tspace     = Float. Sets additional vertical space between nodes and labels for better readability.\n",
        "# node_size  = Integer. Controls the size of each node in the network graph.\n",
        "# font_size  = Integer. Defines the font size for node labels.\n",
        "# pad        = Float. Adjusts padding around the layout for a balanced appearance.\n",
        "# nd_a       = Color string (e.g., '#FF0000'). Specifies the color for the primary node (main author).\n",
        "# nd_b       = Color string (e.g., '#008000'). Specifies the color for secondary nodes (authors with significant links).\n",
        "# nd_c       = Color string (e.g., '#808080'). Specifies the color for other nodes (authors with minor links).\n",
        "# verbose    = Boolean. If True, prints details of each main node and its connections in the console; if False, suppresses this output.\n",
        "bibfile.network_collab( entry     = 'aut',\n",
        "                        tgt       = [],\n",
        "                        topn      = 15,\n",
        "                        rows      = 5,\n",
        "                        cols      = 3,\n",
        "                        wspace    = 0.2,\n",
        "                        hspace    = 0.2,\n",
        "                        tspace    = 0.01,\n",
        "                        node_size = 300,\n",
        "                        font_size = 8,\n",
        "                        pad       = 0.2,\n",
        "                        nd_a      = '#FF0000',\n",
        "                        nd_b      = '#008000',\n",
        "                        nd_c      = '#808080',\n",
        "                        verbose   = False)"
      ],
      "metadata": {
        "id": "aemAaBNAZja8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Collab\n",
        "print(bibfile.ask_gpt_ct)"
      ],
      "metadata": {
        "id": "qUR9etSOZ1QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network - Citation Analisys Between Documents (Blue Nodes) and Citations (Red Nodes).  (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            min_count   = Relationship between nodes that have been cited at least x times;\n",
        "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
        "#            node_size   = Integer. Value for node size;\n",
        "#            font_size   = Integer. Defines the font size for node labels;\n",
        "#            local_nodes = True or False (True -> Only the blue will be displayed, False -> Red and Blue nodes will be displayed)\n",
        "bibfile.network_adj_dir(view = 'notebook', min_count = 7, node_labels = True, node_size = 20, font_size = 10, local_nodes = False)"
      ],
      "metadata": {
        "id": "QxVsdp_E_BoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Table\n",
        "data_nad = bibfile.ask_gpt_nad\n",
        "data_table.DataTable(data_nad, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "PydX9A7v9RSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network - Highlight Citation Analysis Between Documents (Blue Nodes) and Citations (Red Nodes).  (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            article_ids = A list of Blue Nodes. It indicates the documents cited by them;\n",
        "#            ref_ids     = A list of Red Nodes. It indicates the documents that cites them;\n",
        "#            font_size   = Integer. Defines the font size for node labels;\n",
        "#            node_size   = Integer. Value for node size\n",
        "bibfile.find_nodes_dir(view = 'notebook', article_ids = [], ref_ids = [], node_size = 20, font_size = 10)"
      ],
      "metadata": {
        "id": "O8bCgM0L_BoV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network - Highlight Citation Analysis Between Documents (Blue Nodes) and Citations (Red Nodes).  (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            article_ids = A list of Blue Nodes. It indicates the documents cited by them;\n",
        "#            ref_ids     = A list of Red Nodes. It indicates the documents that cites them;\n",
        "#            font_size   = Integer. Defines the font size for node labels;\n",
        "#            node_size   = Integer. Value for node size\n",
        "bibfile.find_nodes_dir(view = 'notebook', article_ids = [], ref_ids = [], node_size = 20, font_size = 10)"
      ],
      "metadata": {
        "id": "vEgzigSL_BoW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network - Local Documents (Only Blue Nodes) Citation History. (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            min_links   = Relationship between nodes that have connected at least x times;\n",
        "#            node_size   = Integer. Value for node size;\n",
        "#            font_size   = Integer. Defines the font size for node labels;\n",
        "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
        "#            chain       = A list of documents. It shows the documents and their citations;\n",
        "#            path       =  Only relevant if 'chain' is not empty. True -> Show only the documents in 'chain'. False -> Show documents and connections.\n",
        "citations = bibfile.network_hist(view = 'notebook', min_links = 0, chain = [], path = False, node_size = 20, font_size = 10, node_labels = True)"
      ],
      "metadata": {
        "id": "Y8O5EjQp_BoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Table\n",
        "data_hist = bibfile.ask_gpt_hist\n",
        "data_table.DataTable(data_hist, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "zzEx_ewz9mBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "citations = bibfile.network_hist(view = 'notebook', min_links = 0, chain = [21, 258], path = False, node_size = 20, font_size = 10, node_labels = True)"
      ],
      "metadata": {
        "id": "fVLRl1mT_BoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "citations = bibfile.network_hist(view = 'notebook', min_links = 0, chain = [3, 8, 56, 84, 120, 248], path = True, node_size = 20, font_size = 10, node_labels = True)"
      ],
      "metadata": {
        "id": "HUtq8EEQ_BoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network - Analyze Hist. Citations\n",
        "# Arguments: min_path_size = Minimum number of elements of a path.\n",
        "hist_paths = bibfile.analyze_hist_citations(citations, min_path_size = 2)"
      ],
      "metadata": {
        "id": "07lG_O0Wwclx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network - Collaboration Analysis Between Authors, Countries, Intitutions Or Adjacency Analysis Between Authors' Keywords or Keywords Plus. (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            adj_type    = 'aut', 'cout', 'inst', 'kwa', or 'kwp'\n",
        "#            min_count   = Relationship between nodes that have connected at least x times;\n",
        "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
        "#            node_size   = -1. (If node_size = -1 then the default value will be used. If node_size > 0 then this new value will be used);\n",
        "#            label_type  = 'id', 'name' (Only meaningfull if node_labels = True. 'id' -> The ID will be displayed; 'name' -> The name will be displayed);\n",
        "#            centrality  = 'degree', 'load', 'betw', 'close', 'eigen', 'katz', 'harmonic', or None. Color nodes according to centrality criterion\n",
        "#                          'degree'   = Degree Centrality\n",
        "#                          'load'     = Load Centrality\n",
        "#                          'betw'     = Betweenness Centrality\n",
        "#                          'close'    = Closeness Centrality\n",
        "#                          'eigen'    = Eigenvector Centrality\n",
        "#                          'katz'     = Katz Centrality\n",
        "#                          'harmonic' = Harmonic Centrality\n",
        "#                           None      = The Community Algorithm, Girvan-Newman, will be used Instead of a Centrality Criterion\n",
        "bibfile.network_adj(view = 'notebook', adj_type = 'aut', min_count = 5, node_labels = True, label_type = 'name', centrality = None)\n",
        "\n",
        "# PS: If a centrality criterion is used then the values can be obtained by the following command:  bibfile.table_centr"
      ],
      "metadata": {
        "id": "h0hMpoQl_BoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Table\n",
        "data_adj = bibfile.ask_gpt_adj\n",
        "data_table.DataTable(data_adj, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "EYLWm2DJ-C4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network - Highlight  Collaboration Analysis Between Authors, Countries, Intitutions Or Adjacency Analysis Between Authors' Keywords or Keywords Plus. (An interactive plot).\n",
        "# Arguments: view      = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            node_ids  = A list of IDs. Only meaningfull if label_type = 'id';\n",
        "#            node_name = A list of Names. Only meaningfull iflabel_type = 'name';\n",
        "#            node_only = True or False (True -> Only the Node will be Highlighted, False -> Node and its Connections will be Highlighted)\n",
        "bibfile.find_nodes(node_ids = [], node_name = ['lam, hon loong'], node_only = False)"
      ],
      "metadata": {
        "id": "Qp1yJdQ__BoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network - Similarity Analysis using coupling or cocitation methods. (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            sim_type    = 'coup', 'cocit' ('coup' -> Coupling Method, 'cocit' -> Cocitation Method)\n",
        "#            node_size   = -1. (If node_size = -1 then the default value will be used. If node_size > 0 then this new value will be used);\n",
        "#            node_labels = True or False (True -> The label IDs will be displayed, False -> Only the nodes will be displayed );\n",
        "#            cut_coup    = Cutoff value for Coupling Method. Only meaninfull if sim_type = 'coup';\n",
        "#            cut_cocit   = Cutoff value for Cocitation Method. Only meaninfull if sim_type = 'cocit'\n",
        "bibfile.network_sim(view = 'notebook', sim_type = 'cocit', node_size = 10, node_labels = True, cut_coup = 0.3, cut_cocit = 5)"
      ],
      "metadata": {
        "id": "s1_c3XYL_BoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Similarity Values\n",
        "data_table.DataTable(bibfile.sim_table, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "JUHMIatj_BoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network - Collaboration Analysis Between Countries using a Map. (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            connections = True or False (True -> Countries connections will be displayed, False -> Countries connections will not be displayed);\n",
        "#            country_lst = Highlight the Connections Between a List of Countries\n",
        "bibfile.network_adj_map(view = 'notebook', connections = True, country_lst = [])"
      ],
      "metadata": {
        "id": "U8dnxtAk_BoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Table\n",
        "data_map = bibfile.ask_gpt_map\n",
        "data_table.DataTable(data_map, num_rows_per_page = 15)"
      ],
      "metadata": {
        "id": "JC_Fyg7z-XDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network - Collaboration Analysis Between Countries using a Map. (An interactive plot).\n",
        "# Arguments: view        = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window);\n",
        "#            connections = True or False (True -> Countries connections will be displayed, False -> Countries connections will not be displayed);\n",
        "#            country_lst = Highlight the Connections Between a List of Countries\n",
        "bibfile.network_adj_map(view = 'notebook', connections = False, country_lst = ['Colombia'])"
      ],
      "metadata": {
        "id": "qoWPjO1Cu2ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Intelligence Analysis\n",
        "---\n",
        "In this section, we will perform AI Analysis"
      ],
      "metadata": {
        "id": "LOhVLGyWd_5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "# Arguments: corpus_type       = 'abs', 'title', 'kwa', or 'kwp';\n",
        "#            stop_words        = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
        "#                                'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
        "#                                'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
        "#                                'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainian\n",
        "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
        "#            model             = Specifies the used AI model. The default value is 'allenai/scibert_scivocab_uncased'\n",
        "bibfile.create_embeddings(stop_words = ['en'], rmv_custom_words = [], corpus_type = 'abs', model = 'allenai/scibert_scivocab_uncased')\n",
        "emb = bibfile.embds"
      ],
      "metadata": {
        "id": "3DQydEMp-AdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP #-1 refers to all outliers and should typically be ignored.\n",
        "# Arguments: stop_words        = A list of stopwords to clean the corpus. ['ar', 'bn', 'bg', 'cs', 'en', 'fi', 'fr', 'de', 'el', 'hi', 'he', 'hu', 'it', 'ja', 'ko',  'mr', 'fa', 'pl', 'pt-br', 'ro', 'ru', 'es', 'sv', 'sk', 'zh', 'th', 'uk'];\n",
        "#                              'ar' = Arabic; 'bn' = Bengali; 'bg' = Bulgarian; 'cs' = Czech; 'en' = English; 'fi' = Finnish; 'fr' = French; 'de' = German; 'el' = Greek; 'he' = Hebrew;'hi' = Hindi; 'hu' = Hungarian; 'it' = Italian;\n",
        "#                              'ja' = Japanese; 'ko' = Korean; 'mr' =  Marathi; 'fa' =  Persian; 'pl' =  Polish; 'pt-br' = Potuguese-Brazilian; 'ro' = Romanian; 'ru' = Russian; 'es' =  Spanish; 'sk' = Slovak; 'sv' = Swedish;\n",
        "#                              'zh' = Chinese; 'th' = Thai; 'uk' = Ukrainianian;   'es' =  Spanish;  'sv' = Swedish\n",
        "#            rmv_custom_words  = A list of custom stopwords to clean the corpus;\n",
        "#            embeddings        = True or False. If True then word embeddings are used to create the topics\n",
        "#            model             = Specifies the used AI model. The default value is 'allenai/scibert_scivocab_uncased'\n",
        "bibfile.topics_creation(stop_words = ['en'], rmv_custom_words = [], embeddings = False, model = 'allenai/scibert_scivocab_uncased')"
      ],
      "metadata": {
        "id": "jm13b3U9Cgz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authors Production per Topic\n",
        "production_topics = bibfile.topics_authors(topn = 15)\n",
        "production_topics"
      ],
      "metadata": {
        "id": "N_DeBZ66b_U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "# Each document Topic\n",
        "topics = bibfile.topics"
      ],
      "metadata": {
        "id": "oKnung64CseX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "# Each document Probability to belong a Topic\n",
        "probs = bibfile.probs"
      ],
      "metadata": {
        "id": "skh6EMP8DGo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window)\n",
        "bibfile.graph_topics_distribution(view = 'notebook')"
      ],
      "metadata": {
        "id": "VsAAbo2eDQ4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window)\n",
        "bibfile.graph_topics(view = 'notebook')"
      ],
      "metadata": {
        "id": "nDj33Pi1DnEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window)\n",
        "bibfile.graph_topics_projection(view = 'notebook')"
      ],
      "metadata": {
        "id": "LpLx6z12DsPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window)\n",
        "bibfile.graph_topics_heatmap(view = 'notebook')"
      ],
      "metadata": {
        "id": "ytTGEDBuDuwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "# Arguments: view = 'notebook', 'browser' ('notebook' -> To plot in your prefered Notebook App. 'browser' -> To plot in your prefered browser window)\n",
        "bibfile.graph_topics_time(view = 'notebook')"
      ],
      "metadata": {
        "id": "LzEkqWe8Hley"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "bibfile.topics_representatives()"
      ],
      "metadata": {
        "id": "GFRl-a5dF51Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "similar_topics, similarity = bibfile.topic_model.find_topics('electre', top_n = 10)\n",
        "for i in range(0, len(similar_topics)):\n",
        "  print('Topic: ', similar_topics[i], 'Correlation: ', round(similarity[i], 3))"
      ],
      "metadata": {
        "id": "Zi-qtpKUGaZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "# Arguments: doc_id = Article ID. For the specified article, this function evaluates how each word in its abstract semantically aligns with all topics in the model.\n",
        "df = bibfile.topics_words(doc_id = 42)\n",
        "df"
      ],
      "metadata": {
        "id": "rf1yY8NeIOPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP\n",
        "bibfile.topic_model.save('my_topic_model')\n",
        "#loaded_topic_model = BERTopic.load('my_topic_model')"
      ],
      "metadata": {
        "id": "nlKvyTO_Hrd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# W2V\n",
        "model, corpus, w_emb, vocab = bibfile.word_embeddings(stop_words        = ['en'],\n",
        "                                                      lowercase         = True,\n",
        "                                                      rmv_accents       = True,\n",
        "                                                      rmv_special_chars = False,\n",
        "                                                      rmv_numbers       = True,\n",
        "                                                      rmv_custom_words  = [],\n",
        "                                                      vector_size       = 100,\n",
        "                                                      window            = 5,\n",
        "                                                      min_count         = 1,\n",
        "                                                      epochs            = 10)"
      ],
      "metadata": {
        "id": "khPf33Ecef4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# W2V - Similarity\n",
        "similarity = bibfile.word_embeddings_sim(model, word_1 = 'optimization', word_2 = 'waste')\n",
        "similarity"
      ],
      "metadata": {
        "id": "qw-faN4OfAx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# W2V - Find Docs\n",
        "results = bibfile.word_embeddings_find_doc(corpus, target_words = ['optimization', 'biomass'])\n",
        "results"
      ],
      "metadata": {
        "id": "C0yeTvRcgBO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# W2V - Operations\n",
        "operations = bibfile.word_embeddings_operations(model, positive = ['mcda', 'group'], negative = ['risk'], topn = 10)\n",
        "operations"
      ],
      "metadata": {
        "id": "xTlaZVK6fIi1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# W2V - Operations Plot.\n",
        "bibfile.plot_word_embeddings(model,\n",
        "                             view      = 'notebook',\n",
        "                             positive  = [ ['mcda', 'group'], ['decision'],             ['group']],\n",
        "                             negative  = [ ['risk'],          ['research', 'analysis'], []       ],\n",
        "                             topn      = 3,\n",
        "                             node_size = 10,\n",
        "                             font_size = 14)"
      ],
      "metadata": {
        "id": "OyWQfo73fWrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP - Abstractive Summarization\n",
        "# Arguments: article_ids = A list of documents to perform an abstractive summarization with the available abstracts. If the list is empty then all documents will be used\n",
        "#            model_name  = Available pre-trained models. Complete list is available at  https://huggingface.co/models?pipeline_tag=summarization&sort=downloads&search=pegasus\n",
        "abs_summary = bibfile.summarize_abst_peg(article_ids = [218, 28, 212], model_name = 'google/pegasus-xsum')"
      ],
      "metadata": {
        "id": "pBYNqxljhSjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP - Check Abstractive Summarization\n",
        "print(textwrap.fill(abs_summary, 150))"
      ],
      "metadata": {
        "id": "0_ugzx3sH1iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP - Abstractive Summarization - chatGPT\n",
        "\n",
        "# OBS 1: Requires the user to have an **API key** (https://platform.openai.com/account/api-keys))\n",
        "# OBS 2: The limit of characters is 4097 per request\n",
        "\n",
        "# Arguments: article_ids   = A list of documents to perform an abstractive summarization with the available abstracts. If the list is empty then all documents will be used\n",
        "#            join_articles = If False then the abstracts will be analyzed separately. If True then the abstracts will be concate in a single text\n",
        "#            api_key       = 'your_api_key_here'. Insert your personal API key (https://platform.openai.com/account/api-keys)\n",
        "#            model         = Specifies the AI model used for text generation. The default value is \"text-davinci-003\"\n",
        "#            query         = Ask chatGPT what you want to do with the abstracts. The default query is: 'from the following scientific abstracts, summarize the main information in a single paragraph using around 250 words'\n",
        "abs_summary_chat = bibfile.summarize_abst_chatgpt(article_ids = [], join_articles = True, api_key = 'your_api_key_here', query = 'from the following scientific abstracts, summarize the main information in a single paragraph using around 250 words', model = 'gpt-4')"
      ],
      "metadata": {
        "id": "tuIyocQkWpsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP - Check Abstractive Summarization\n",
        "print(textwrap.fill(abs_summary_chat, 250))"
      ],
      "metadata": {
        "id": "QblNnsZFXZ5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP - Extractive Summarization\n",
        "# Arguments: article_ids = A list of documents to perform an extractive summarization with the available abstracts. If the list is empty then all documents will be used\n",
        "ext_summary = bibfile.summarize_ext_bert(article_ids = [])"
      ],
      "metadata": {
        "id": "aywSSF4BH1sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP - Check Extractive Summarization\n",
        "print(textwrap.fill(ext_summary, 150))"
      ],
      "metadata": {
        "id": "2qW_Ek64hd8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correction and Data Manipulation\n",
        "---\n",
        "In this section, we will show how to Correct & Manipulate your Data"
      ],
      "metadata": {
        "id": "DRNqW8aMe3RS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx74Fohnjk3x"
      },
      "outputs": [],
      "source": [
        "# Filter the .bib File\n",
        "# Arguments: document = A list of documents to retain. The other documents will be deleted\n",
        "#            doc_type = A list of doc types. Check the 'report' to select the available types;\n",
        "#            year_str = An integer the determines the starting year of collection -1 = All years;\n",
        "#            year_end = An integer the determines the ending year of collection   -1 = All years;\n",
        "#            sources  = A list of sources. Check the cell '# Check Sources IDs' to select the available types;\n",
        "#            core     = A integer (-1, 1, 2, 3, 12, or 23) -1 = All sources, 1 = Bradford core 1, 2 = Bradford core 2, 3 = Bradford core 3, 12 = Bradford core 1 and 2, 23 = Bradford core 2 and 3;\n",
        "#            country  = A list of countries. Check the cell '# Check Countries IDs' to select the available types;\n",
        "#            language = A list of languages. Check the 'report' to select the available types\n",
        "#            abstract = True or False. True removes UNKNOW values from the abstract.\n",
        "bibfile.filter_bib(documents = [], doc_type = [], year_str = -1, year_end = -1, sources = [], core = -1, country = [], language = [], abstract = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct the .bib File\n",
        "# Arguments: get         = A list of the current name(s);\n",
        "#            replace_for = A string. This string will replace all matchs from the 'get' argument list\n",
        "bibfile.merge_author(get = [], replace_for = 'name')\n",
        "bibfile.merge_institution(get = [], replace_for = 'name')\n",
        "bibfile.merge_country(get = [], replace_for = 'name')\n",
        "bibfile.merge_language(get = [], replace_for = 'name')\n",
        "bibfile.merge_source(get = [], replace_for = 'name')\n",
        "bibfile.merge_reference(get = [], replace_for = 'name')"
      ],
      "metadata": {
        "id": "wuQ854UdlzAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Work with modified .bib File\n",
        "\n",
        "# 1) Input a .bib to instantiate a class => bibfile  = pbx_probe(file_bib = file_name, db = database, del_duplicated = True)\n",
        "# 2) Then made the modifications and save the data externally => bibfile.save_database(sep = '\\t', name = 'data.csv')\n",
        "# 3) To load the modified .bib file ( 'data.csv') => bibfile.load_database(name = 'data.csv')\n",
        "\n",
        "# PS: Even with the saved 'data.csv', you always need to do step 1, so to be fast, have any small .bib file in hand.\n",
        "# Then, you can go directly to step 3, and your saved .bib ('data.csv') will replace the small .bib"
      ],
      "metadata": {
        "id": "FOFb5DU7WD5M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}